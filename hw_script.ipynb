{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Часть первая, с блекджеком и стратегиями"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Правила блекджека достаточно просты;\n",
    "давайте начнём с самой базовой версии, которая реализована в OpenAI Gym:\n",
    "*  численные значения карт равны от 2 до 10 для карт от двойки до десятки, 10 для валетов, дам и королей;\n",
    "* туз считается за 11 очков, если общая сумма карт на руке при этом не превосходит 21 (по-английски в этом случае говорят, что на руке есть usable ace), и за 1 очко, если превосходит;\n",
    "*  игроку раздаются две карты, дилеру — одна в открытую и одна в закрытую;\n",
    "* игрок может совершать одно из двух действий:\n",
    "** hit  — взять ещё одну карту;\n",
    "** stand — не брать больше карт;\n",
    "* если сумма очков у игрока на руках больше 21, он проигрывает (bust);\n",
    "* если игрок выбирает stand с суммой не больше 21, дилер добирает карты, пока сумма карт в его руке меньше 17;\n",
    "* после этого игрок выигрывает, если дилер либо превышает 21, либо получает сумму очков меньше, чем сумма очков у игрока; при равенстве очков объявляется ничья (ставка возвращается);\n",
    "* в исходных правилах есть ещё дополнительный бонус за natural blackjack: если игрок набирает 21 очко с раздачи, двумя картами, он выигрывает не +1, а +1.5 (полторы ставки).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 1\n",
    "Рассмотрим очень простую стратегию: говорить stand, если у нас на руках комбинация в 19, 20 или 21 очко, во всех остальных случаях говорить hit.\n",
    "Используйте методы Монте-Карло, чтобы оценить выигрыш от этой стратегии.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Card Values:\n",
    "\n",
    "    Face cards (Jack, Queen, King) have a point value of 10.\n",
    "\n",
    "    Aces can either count as 11 (called a ‘usable ace’) or 1.\n",
    "\n",
    "    Numerical cards (2-9) have a value equal to their number.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from blackjack import BlackjackEnv\n",
    "from functools import partialmethod"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "blackjack.BlackjackEnv"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = BlackjackEnv()\n",
    "env.natural = True\n",
    "type(env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def play_game(env, strategy):\n",
    "    # sum_hand, dealer, usable_ace\n",
    "    observation = env.reset()[0]\n",
    "    terminated = False\n",
    "    G = 0\n",
    "    while not terminated:\n",
    "        # There are two actions: stick (0), and hit (1).\n",
    "        action = strategy(observation)\n",
    "        observation, reward, terminated, _, _ = env.step(action)\n",
    "        G += reward\n",
    "    return G\n",
    "\n",
    "def simple_strategy(observation):\n",
    "    usable_ace = observation[0] + 10 * int(observation[2]) not in (19, 20, 21)\n",
    "    not_usable_ace = observation[0] not in (19, 20, 21)\n",
    "    return int(usable_ace or not_usable_ace)\n",
    "\n",
    "\n",
    "def play_games(env, strategy=simple_strategy, n=100_000):\n",
    "    rewards = []\n",
    "    for _ in tqdm(range(n)):\n",
    "        reward = play_game(env, strategy)\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4465.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "-0.17845"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_games(env, strategy=simple_strategy, n=10_000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Данная стратегия проигрышная, мы в минусе"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание 2\n",
    "Реализуйте метод обучения с подкреплением без модели (можно Q-обучение, но рекомендую попробовать и другие, например Monte Carlo control) для обучения стратегии в блекджеке, используя окружение BlackjackEnv из OpenAI Gym."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "S = []\n",
    "for i in range(1 + 1 + 2, 11 + 11 + 10):\n",
    "    for j in range(1, 11):\n",
    "        for k in (True, False):\n",
    "            if not k or (k and 12 <= i <= 21):\n",
    "                S.append((i, j, k))\n",
    "state_index = {state: i for i, state in enumerate(S)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def get_action_Q(Q):\n",
    "    return np.argmax(Q, axis=1)\n",
    "\n",
    "def play_game_Q(env, Q, alpha=0.05, epsilon=0.1, gamma=1):\n",
    "    observation = env.reset()[0]\n",
    "    index = state_index[observation]\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        action = get_action_Q(Q)[index] if random.random() < (1 - epsilon) else random.choice((0, 1))\n",
    "        observation, reward, terminated, _, _ = env.step(action)\n",
    "        index_new = state_index[observation]\n",
    "        Q[index, action] = Q[index, action] + alpha * (reward + gamma * max(Q[index_new]) - Q[index, action])\n",
    "        index = index_new\n",
    "    return Q\n",
    "\n",
    "def q_learning(env, Q, games=10_000, alpha=0.01, epsilon=0.01, gamma=1):\n",
    "    for _ in tqdm(range(games)):\n",
    "        Q = play_game_Q(env=env, Q=Q, alpha=alpha, epsilon=epsilon, gamma=gamma)\n",
    "    return Q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3706.33it/s]\n"
     ]
    }
   ],
   "source": [
    "num_comb = len(list(product(range(1 + 1 + 2, 11 + 11 + 10), range(1, 11), (True, False))))\n",
    "Q = np.zeros((len(S), 2))\n",
    "Q = q_learning(env, Q, games=10_000, alpha=0.01, epsilon=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 3963.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "-0.06445"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Q_strategy(observation):\n",
    "    return np.argmax(Q, axis=1)[state_index[observation]]\n",
    "\n",
    "play_games(env, strategy=Q_strategy, n=10_000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Уже лучше, но всё равно в минусе :("
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 3\n",
    "Сколько выигрывает казино у вашей стратегии? Нарисуйте графики среднего дохода вашего метода (усреднённого по крайней мере по 100000 раздач, а лучше больше) по ходу обучения. Попробуйте подобрать оптимальные гиперпараметры."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-12-30 18:14:17,697]\u001B[0m A new study created in memory with name: no-name-41234c7b-bf4f-4418-a88e-a9a4e938e07b\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float('alpha', 0, 1)\n",
    "    epsilon = trial.suggest_float('epsilon', 0, 0.1)\n",
    "    Q = np.zeros((len(S), 2))\n",
    "    Q = q_learning(env, Q, games=10_000, alpha=alpha, epsilon=epsilon)\n",
    "    return play_games(env, strategy=Q_strategy, n=10_000)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1_000, n_jobs=-1, show_progress_bar=False)\n",
    "\n",
    "study.best_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
